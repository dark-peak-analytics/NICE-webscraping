
# This code runs the webscraping of NICE published guidance. Please see comments throughout.
# There are no API setups or LLMs or anything that requires a license.
# This code may be forked and amended as needed.
# email rhart@darkpeakanalytics.com if there are any questions

# Load libraries -----------------

library(rvest)
library(pdftools)
library(tidyverse)
library(svMisc)
library(readr)

#'SETTINGS --------------
#' Specify whether to run the model from the beginning or just update it with new entries

Run_All_Iterations <- FALSE

# Load table with all guidance to be examined ------------------
#' Thie has been generated by visiting https://www.nice.org.uk/guidance/published and selecting
#' to view all results, copying to clipboard, and adding to csv

Guidance_full_df <- read.csv("Guidance.csv")
nrow(Guidance_full_df)

#' Where the webscraping has not been successful and guidance has had to be manually checked
#' whilst developing this code), these are stored in Excel and loaded in
manual_checks <- read.csv("ManualDetails.csv")

#' Filter to only examine technology appraisals
Guidance_TAs_df <- Guidance_full_df[grepl("TA",Guidance_full_df$Reference.number),]
nrow(Guidance_TAs_df)

# Filter out all terminated appraisals
Guidance_TAs_df$Title <- iconv(Guidance_TAs_df$Title, from = "ISO-8859-1", to = "UTF-8")
Guidance_TAs_df <- Guidance_TAs_df[!grepl("terminated appraisal",Guidance_TAs_df$Title),]
nrow(Guidance_TAs_df)

# Create the web addresses for evidence
Guidance_TAs_df$address <- paste0("https://www.nice.org.uk/guidance/",Guidance_TAs_df$Reference.number,"/evidence")


# If all iterations are to be run, then Run_All_Iterations is set to TRUE and the Guidance_TAs_list
# is created as a shell for populating.
# If only new entries are to be run, the Guidance_TAs_list is created and populated with the existing
# data. Only those in the Guidance_TAs_list without data form a vector of offsets to run scraping
if(Run_All_Iterations){
  # Create Guidance_TAs_list with only the 'Overview' inside, for the scraping function to use
  Guidance_TAs_list  <- lapply(1:nrow(Guidance_TAs_df), function(row) {
    out <- list(Guidance_TAs_df[row, ])
    names(out) <- "overview"
    return(out)
  })
  strt <- 1
  end <- length(Guidance_TAs_list)
  vector_run <- strt:end
} else {
  Guidance_TAs_import <- readRDS("Guidance_TAs_list.rds")
  imported_refN <- as.data.frame(t(sapply(1:length(Guidance_TAs_import), function(N){
    c(N, Guidance_TAs_import[[N]]$overview$Reference.number)
  })))
  colnames(imported_refN) <- c("N","Reference.number")

  # Create Guidance_TAs_list and populate with pre-scraped data. If there is no
  # entry in the pre-scraped data, then the 'Overview' is added to list
  Guidance_TAs_list  <- lapply(1:nrow(Guidance_TAs_df), function(row) {
    if(Guidance_TAs_df[row, "Reference.number"] %in% imported_refN$Reference.number){
      import_offset <- as.numeric(imported_refN$N[which(Guidance_TAs_df[row, "Reference.number"] == imported_refN$Reference.number)])
      return(Guidance_TAs_import[[import_offset]])
    } else {
      out <- list(Guidance_TAs_df[row, ])
      names(out) <- "overview"
      return(out)
    }
  })

  # Determin which of Guidance_TAs_list list offsets need to be scraped in the update
  update_iterations <- sapply(1:nrow(Guidance_TAs_df), function(row) {
    if(Guidance_TAs_df[row, "Reference.number"] %in% imported_refN$Reference.number){
      0
    } else {
      1
    }
  })

  if(all(update_iterations ==0)){
    warning("No updates needed")
    strt <- 0
    end <- 0
    vector_run <- 0
  } else {
    # Create vector of offsets of Guidance_TAs_list to run
    vector_run <- which(update_iterations ==1)
    strt = min(vector_run)
    end = max(vector_run)
  }

}

#SCRAPING FUNCTION ##########
pb = txtProgressBar(min = strt, max = end, initial = 0, style = 3)

for(x in vector_run){

  setTxtProgressBar(pb,x)
  print(x)
  url <- Guidance_TAs_list[[x]]$overview$address
  page <- read_html(url)

  # Result is unconfirmed until entered otherwise
  Guidance_TAs_list[[x]]$result <- "Unconfirmed"

  # Gather disease
  element_a_vec <- page %>% html_elements("a") %>% html_text2()
  Guidance_TAs_list[[x]]$condition <- element_a_vec[4]
  Guidance_TAs_list[[x]]$disease   <- element_a_vec[5]


  # make a dataframe of available evidence
  # extract all links
  links_vec <- html_attr(html_nodes(page, "a"), "href")
  data_track_vec <- page %>% html_elements("a") %>% html_attr("data-track")

  names_resources_vec <- element_a_vec[which(data_track_vec == "resourcedownload")]
  links_resources_vec <- paste0("https://www.nice.org.uk",links_vec[which(data_track_vec == "resourcedownload")])

  # if there are no resources then the function returns  "No evidence available"
  if(length(names_resources_vec)==0){
    Guidance_TAs_list[[x]]$report <-  "Scraped"
    Guidance_TAs_list[[x]]$result <-  "No evidence available"
    next
  }

  # Extracting the resources and the date into a dataframe
  resources_vec_date <- page %>% html_elements("time") %>% html_attr("datetime")

  resource_df <- data.frame(
    title = names_resources_vec,
    link = links_resources_vec,
    date = as.Date.character(resources_vec_date)[(length(resources_vec_date)-length(names_resources_vec)+1):length(resources_vec_date)]
  )

  # Remove any resource that is not a pdf
  resource_df <- resource_df[grepl("pdf",resource_df$link),]

  # Identifying size of pdfs
  PDFstring <- str_locate(resource_df$title,"PDF ")
  PDFstring[which(is.na(PDFstring))] <- 0

  #find if doc is in MB or kB
  sizeunit <- substr(resource_df$title,str_length(resource_df$title)-2,str_length(resource_df$title)-1)
  sizemult <- ifelse(sizeunit=="MB",1,1/1000)

  #Identify size of files
  sizefile <- as.numeric(substr(resource_df$title,str_locate(resource_df$title,"PDF ")[,"end"]+1,str_length(resource_df$title)-3))
  sizefile[which(is.na(sizefile))] <- 0
  resource_df$sizeMB <- sizefile*sizemult

  # Order the resources by size (largest) and most recent to select most likely resource containing a version of Doc B
  resource_df <- resource_df[order(resource_df$date, decreasing = TRUE),]
  resource_df <- resource_df[order(resource_df$sizeMB, decreasing = TRUE),]

  # output the table
  Guidance_TAs_list[[x]]$resources <- resource_df

  # We only want to extract from a pdf if it has one or more of these terms in it
  Checkvec   <- c(
    "Committee Papers",
    "Document B",
    "evidence submission"
  )

  # Go through each of the resources and pick the first one that contains one of the terms
  for(nlink in 1:nrow(Guidance_TAs_list[[x]]$resources)){
    linkout <- NULL
    extract_text <- pdf_text(Guidance_TAs_list[[x]]$resources$link[nlink]) %>%
      readr::read_lines()

    Checkcount <- colSums(sapply(Checkvec, function(i)
      stringr::str_count(extract_text, i)))

    if (sum(Checkcount) == 0) {
      Sys.sleep(2)
      next
    } else {
      linkout <- list(url = Guidance_TAs_list[[x]]$resources$link[nlink],
                      counts = Checkcount)
      break
    }
  }

  Guidance_TAs_list[[x]]$resource_selected <- linkout

  if(is.null(linkout)){
    Guidance_TAs_list[[x]]$report <-  "Scraped"
    Guidance_TAs_list[[x]]$result <-  "No evidence available"
    next
  }

  if(Guidance_TAs_list[[x]]$overview$Reference.number %in% manual_checks$Reference.number){
    Guidance_TAs_list[[x]]$report <- manual_checks$Report[which(manual_checks$Reference.number ==  Guidance_TAs_list[[x]]$overview$Reference.number)]
    Guidance_TAs_list[[x]]$result <- manual_checks$Result[which(manual_checks$Reference.number ==  Guidance_TAs_list[[x]]$overview$Reference.number)]
    next
  }


  refined_text <- ""

  # Terms of interest vectors - these terms are scraped for and used in different ways below
  # Some terms have the first letter removed to remove issues with capitalisation vs non-capitalisation
  TOIvec1 <- c("Microsoft","MS Excel","in Excel","VBA"," R ","TreeAge")
  TOIvec2 <- c("built","build"," constructed","developed", "conducted", "programmed", "implemented","perform","utilises","created")
  TOIvec3 <- c("arametric","flexsurvreg","egression","NMA","nma","mixture","R mono","Bucher","IPD","lgorithm","AIC","cox","ropensity","digitised","fit","extrapolations","RPSFT","extract","performing PSA","Appendix","MAIC","eta-analyses","aggregate","mixed","IPCW")
  TOIvec4 <- c("ost","conomic","odel")

  # Find all instances of TOIvec1 (software) in the document
  lines_extract_TOIvec1 <- unique(unlist(lapply(TOIvec1, function(i) {
    which(grepl(i, extract_text))
  })))

  if (length(lines_extract_TOIvec1) == 0) {
    Guidance_TAs_list[[x]]$report <-  "Scraped"
    Guidance_TAs_list[[x]]$result <-  "Not stated"
    next
  }


  # Once a software is found then extract the relevant verbs of interest from section (TOIvec2)
  lines_extract_TOIvec2 <- unique(unlist(lapply(TOIvec2, function(i) {
    which(grepl(i, extract_text))
  })))

  if (length(lines_extract_TOIvec2) == 0) {
    Guidance_TAs_list[[x]]$report <-  "Scraped"
    Guidance_TAs_list[[x]]$result <-  "Not stated"
    next
  }

  #extract lines only where lines_extract_TOIvec1 and lines_extract_TOIvec2 are within 2 lines of eachother
  line_vec <- unique(unlist(lapply(lines_extract_TOIvec2, function(i) {
    a <- i < lines_extract_TOIvec1 + 2
    b <- i < lines_extract_TOIvec1 - 2
    lines_extract_TOIvec1[which(a != b)]
  })))

  if (length(line_vec) == 0) {
    Guidance_TAs_list[[x]]$report <-  "Scraped"
    Guidance_TAs_list[[x]]$result <-  "Not stated"
    next
  }

  #Extract the lines of interest and output them into Guidance_TAs_list
  # these can be then manually extracted for QC checks/confirmation
  refined_lines <- lapply(line_vec, function(i) {
    extract_text[max(1, i - 2):min(length(extract_text), i + 2)]
  })

  Guidance_TAs_list[[x]]$refined_lines <- refined_lines

  # Removing noise from statistical analysis in R
  # If any statistical terms are within the sections of
  # refined lines text then the lines are removed
  refined_lines <- lapply(refined_lines, function(i) {
    temp <- sapply(TOIvec3, function(j) {
      grepl(j, paste(i, collapse = " "))
    })
    if (any(temp)) {
      return(NULL)
    } else{
      return(i)
    }
  })

  # refine further to remove noise
  # Return only those that mention any of the terms in TOIvec4
  refined_lines <- lapply(refined_lines, function(i) {
    temp <- sapply(TOIvec4, function(j) {
      grepl(j, paste(i, collapse = " "))
    })
    if (any(temp)) {
      return(i)
    } else{
      return(NULL)
    }
  })

  # unlist the refined lines and accumulate as single chunk of text
  refined_text <- paste(unique(unlist(refined_lines)), collapse = " ")


  if (refined_text == "") {
    Guidance_TAs_list[[x]]$report <-  "Scraped"
    Guidance_TAs_list[[x]]$result <-  "Not stated"
    next
  }

  # Count the instances of TOIvec1
  counts <- sapply(TOIvec1, function(x) stringr::str_count(refined_text, x))

  Guidance_TAs_list[[x]]$counts <- counts

  # Return the result
  if(sum(counts[1:3]) == 0){
    if(length(which(counts != 0))==1){
      Guidance_TAs_list[[x]]$report <-  "Scraped"
      Guidance_TAs_list[[x]]$result <- gsub(" ","",names(counts)[which(counts != 0)]) # R or TreeAge -  these are subsequently confirmed
    } else {
      Guidance_TAs_list[[x]]$report <-  "Scraped"
      Guidance_TAs_list[[x]]$result <-  "To Confirm - Multiple" # These are then confirmed manually and re-run
    }
  } else if(sum(counts[1:3]) > 0 & counts[4] > 0){
    Guidance_TAs_list[[x]]$report <-  "Scraped"
    Guidance_TAs_list[[x]]$result <-  "VBA" #This is then subsequently verified
  } else if(sum(counts[4:length(counts)]) != 0){
    Guidance_TAs_list[[x]]$report <-  "Scraped"
    Guidance_TAs_list[[x]]$result <-  "To Confirm - Multiple" # These are then confirmed manually and re-run
  } else {
    Guidance_TAs_list[[x]]$report <-  "Scraped"
    Guidance_TAs_list[[x]]$result <-  "Microsoft Excel" # Microsoft Excel returned - this is not manually verified
  }


  # Adding this in to avoid DDOS issues
  Sys.sleep(2)

}



# SAVE UPDATES -------------
#' Uncomment to save
#saveRDS(Guidance_TAs_list, file = "Guidance_TAs_list.rds")


#
results <- unlist(lapply(Guidance_TAs_list, function(list_obj){
  list_obj$result
}))
# Check results
which(results == "To Confirm - Multiple")
which(results == "R")
which(results == "VBA")


Guidance_TAs_list[[x]]$refined_lines
Guidance_TAs_list[[x]]$overview
Guidance_TAs_list[[x]]$counts



